{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- states\n",
    "- actions\n",
    "- rewards\n",
    "- discount factor (gamma or g)\n",
    "- return\n",
    "\t- R1 + g\\*R2 + g^2\\*R3 + g^3*R4 + ...\n",
    "- policy (pi)\n",
    "\t- pi(state) => action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Q function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q(s, a) = Return if you\n",
    "\t- start in state s\n",
    "\t- take action a (once)\n",
    "\t- then behave optimally after that\n",
    "- Best possible return from state s is max Q(s, a)\n",
    "- Best possible action in state s is the action that gives max Q(s, a)\n",
    "- Bellman equation\n",
    "\t- Q(s, a) = R(s, a) + g * max Q(s', a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refinements to Algorithm: Mini Batching and Soft Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of training supervised models in neural networks with 1,000,000 dataset as a whole, train the neural network with small mini-batches of say 1000 random sample datas each time to reduce the training time.\n",
    "\n",
    "\n",
    "- Instead of altering the parameters in the neural networks completely for every iterations, use small learning rate (like 0.001) and change the parameters gradually for better accuracy during every iteration.\n",
    "\n",
    "Wrong way\n",
    "- w = w_new\n",
    "\n",
    "Correct way\n",
    "- w = w + learning_rate * change_w  (i.e: dJ/dw)\n",
    "- w = w + learning_rate * (w_new - w)\n",
    "- w = learning_rate * w_new + (1 - learning_rate) * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Q Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "num_states = 6\n",
    "num_actions = 2\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.5\n",
    "exploration_prob = 0.2\n",
    "num_episodes = 1000\n",
    "\n",
    "# Define the transition rewards (R)\n",
    "rewards = np.array([100, 0, 0, 0, 0, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "q_table[0] = rewards[0]\n",
    "q_table[num_states - 1] = rewards[num_states - 1]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\tstate = random.randint(0, num_states - 1)\n",
    "\t\n",
    "\twhile state not in [0, num_states - 1]:  # Continue until reaching termination states\n",
    "\t\t# Exploration vs. Exploitation\n",
    "\t\tif random.uniform(0, 1) < exploration_prob:\n",
    "\t\t\taction = random.randint(0, num_actions - 1)\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state, :])\n",
    "\n",
    "\t\t# Update Q-value using the Q-learning formula\n",
    "\t\tnext_state = (state - 1) if action == 0 else (state + 1)\n",
    "\t\treward = rewards[state]\n",
    "\t\tq_table[state, action] += learning_rate * (reward + \\\n",
    "\t\t\tdiscount_factor * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "\n",
    "\t\tstate = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[100.   100.  ]\n",
      " [ 50.    12.5 ]\n",
      " [ 25.     6.25]\n",
      " [ 12.5   10.  ]\n",
      " [  6.25  20.  ]\n",
      " [ 40.    40.  ]]\n",
      "Optimal path: [3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Print the learned Q-table\n",
    "print(\"Learned Q-table:\")\n",
    "print(q_table)\n",
    "\n",
    "# Testing the learned policy\n",
    "current_state = 3\n",
    "path = [current_state]\n",
    "\n",
    "while current_state not in [0, num_states-1]:\n",
    "\taction = np.argmax(q_table[current_state, :])\n",
    "\tcurrent_state = (current_state - 1) if action == 0 else (current_state + 1)\n",
    "\tpath.append(current_state)\n",
    "\n",
    "print(\"Optimal path:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deep Q Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training deep neural networks to approximate the optimal Q function for continuos valued fucntional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Supervised(classification, regression), Unsupervised(clustering, anomaly detection), cnn, computer vision, rnn (nlp, sentiment), reinforcement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
